{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a demonstration on how to clean a set of documents to prepare them for analysis. It was designed by Greta Frei to analyze articles published by the National Catholic Welfare Council from 1920 - 1950 hosted online by the Catholic News Archive. Questions can be directed to mrauch2@bu.edu. Cleaning is particular to a dataset, so not all the steps here may be necessary for your application or additional steps may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OCR for the articles downloaded from CNA contains HTML tags. Each paragraph in the text references a different article on the page, not a different paragraph in a single article. Therefore, this method removes the html tags and keeps only paragraphs that contain China or Chinese. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean HTML tags and filter text\n",
    "def clean_html_tags(text):\n",
    "    global paragraphs_kept\n",
    "    global paragraphs_total\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    cleaned_text = []\n",
    "    # only keep paragraphs that reference China\n",
    "    for paragraph in paragraphs:\n",
    "        paragraphs_total += 1\n",
    "        if re.search(r'\\b(?:china|chinese)\\b', paragraph.get_text(), re.IGNORECASE):\n",
    "            cleaned_text.append(paragraph.get_text())\n",
    "            paragraphs_kept += 1\n",
    "    return ' '.join(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code reads in the original scrapped articles, filters out all other publications besides the NCWC bulliten (which has the id of cns), and filters by date. It then cleans up the OCR text by running the html cleaner method defined above. This code assumes that the data has already been downloaded and stored in a csv file called gathered_sections.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cns articles: 19959\n",
      "total cns articles in year range: 8428\n",
      "Total paragraphs: 52761 \n",
      "Paragraphs kept: 9443\n"
     ]
    }
   ],
   "source": [
    "# Read the dataframe\n",
    "df = pd.read_csv('gathered_sections.csv', index_col='section_id')\n",
    "\n",
    "# Filter the dataframe\n",
    "df = df[(df['publication_id'] == \"cns\")]\n",
    "print(f'total cns articles: {len(df.index)}')\n",
    "\n",
    "# Fix the dates\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['publication_year'] = df['date'].dt.year\n",
    "\n",
    "# Limit to articles published between 1920 and 1950\n",
    "df = df[(df['publication_year'] >= 1920) & (df['publication_year'] <= 1950)]\n",
    "print(f'total cns articles in year range: {len(df.index)}')\n",
    "\n",
    "paragraphs_kept = 0\n",
    "paragraphs_total = 0\n",
    "\n",
    "# Apply the HTML cleaning function\n",
    "df['ocr_text_cleaned'] = df['ocr_text'].apply(clean_html_tags)\n",
    "# Drop the original OCR text column\n",
    "\n",
    "df.drop(columns=['ocr_text'], inplace=True)\n",
    "\n",
    "# Save the cleaned dataframe\n",
    "df.to_csv('cns_articles_cleaned.csv')\n",
    "print(f'Total paragraphs: {paragraphs_total} \\nParagraphs kept: {paragraphs_kept}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dictionary is a list of terms to be replaced in each article. These terms include common abbreviations of countries and various Catholic organizations. They were developed organically in an interative process of inspecting output from the cleaning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace apprevations\n",
    "replace_terms = {\n",
    "    \" chinee \": \" china \",\n",
    "    \"catholic u \": \"fujen \",\n",
    "    \"catholic u.\": \"fujen\",\n",
    "    \"catholic u. of peking\": \"fujen\",\n",
    "    \"peking catholic u.\": \"fujen\",\n",
    "    \"peking cath. u.\": \"fujen\",\n",
    "    \"peking u.\": \"fujen\",\n",
    "    \"catholic u. of pekin \": \"fujen \",\n",
    "    \"catholic university of pekin \": \"fujen \",\n",
    "    \"catholic university of peking\": \"fujen \",\n",
    "    \"catholic university at peking\": \"fujen \",\n",
    "    \"news service\": \"news\",\n",
    "    \"national catholic welfare council\": \"ncwc\",\n",
    "    \"national catholic welfare committee\": \"ncwc\",\n",
    "    \"national catholic welfare committee\": \"ncwc\",\n",
    "    \"national catholic war committee\": \"ncwc\",\n",
    "    \"national catholic war council\": \"ncwc\",\n",
    "    \"n.c.w.c.\": \"ncwc\",\n",
    "    \"n c w c\": \"ncwc\",\n",
    "    \"n. c. w. c.\": \"ncwc\",\n",
    "    \"n. c. iv. c.\": \"ncwc\",\n",
    "    \"n. c. if. c.\": \"ncwc\",\n",
    "    'ncwc': \"ncwc\",\n",
    "    'national council of catholic women': \"nccw\",\n",
    "    'nccw': \"nccw\",\n",
    "    'n.c.c.w.': \"nccw\",\n",
    "    'cwc': \"ccw\",\n",
    "    'c.w.c.': \"ccw\",\n",
    "    'c. w. c.': \"ccw\",\n",
    "    'c w c': \"ccw\",\n",
    "    \"u. n.\": \"unitednations\",\n",
    "    \"u.n.\": \"unitednations\",\n",
    "    \" un \": \" unitednations \",\n",
    "    \" united nations \": \" unitednations \",\n",
    "    'o.p.': \"dominican\",\n",
    "    'o. p.': \"dominican\",\n",
    "    'c.s.c.': \"holycross\",\n",
    "    'c. s. c.': \"holycross\",\n",
    "    'c s c': \"holycross\",\n",
    "    \" cdl \": \" cardinal \",\n",
    "    \" bp \": \" bishop \",\n",
    "    \" st. \": \" saint \",\n",
    "    \" st \": \" saint \",\n",
    "    \" sr \": \" sister \",\n",
    "    \" sr. \": \" sister \",\n",
    "    \"chinese\": \"china\",\n",
    "    \"japanese\": \"japan\",\n",
    "    \"vietnamese\": \"vietnam\",\n",
    "    \"hong kong\": \"hongkong\",\n",
    "    \"u.s.a.\": \"usa\",\n",
    "    \"u.s.\": \"usa\",\n",
    "    \"u. s.\": \"usa\",\n",
    "    \"united states of america\": \"usa\",\n",
    "    \"united states\": \"usa\",\n",
    "    \"americans\": \"usa\",\n",
    "    \"american\": \"usa\",\n",
    "    \"america\": \"usa\",\n",
    "    \"america\": \"usa\",\n",
    "    \"russians\": \"russia\",\n",
    "    \"russian\": \"russia\",\n",
    "    \"african\": \"africa\",\n",
    "    \"indian\": \"india\",\n",
    "    \"irish\": \"ireland\",\n",
    "    \"belgian\": \"belgium\",\n",
    "    \"canadian\": \"canada\",\n",
    "    \"asians\": \"asia\",\n",
    "    \"asian\": \"asia\",\n",
    "    \"koreans\": \"korea\",\n",
    "    \"korean\": \"korea\",\n",
    "    \"spanish\": \"spain\",\n",
    "    \"germans\": \"germany\",\n",
    "    \"german\": \"germany\",\n",
    "    \"french\": \"france\",\n",
    "    \"british\": \"britain\",\n",
    "    \"cubans\": \"cuba\",\n",
    "    \"cuban\": \"cuba\",\n",
    "    \"polish\": \"poland\",\n",
    "    \"europeans\": \"europe\",\n",
    "    \"european\": \"europe\",\n",
    "    \"italians\": \"italy\",\n",
    "    \"italian\": \"italy\",\n",
    "    \"catholics \": \"catholic \",\n",
    "    \"tibetan\": \"tibet\",\n",
    "    \"mary knoller\": \"maryknoll\",\n",
    "    \"mary knoll\": \"maryknoll\",\n",
    "    \"maryknoller\": \"maryknoll\",\n",
    "    \"mexicans\": \"mexico\",\n",
    "    \"mexican\": \"mexico\",\n",
    "    \"haitian\": \"haiti\",\n",
    "    \"haitians\": \"haiti\",\n",
    "    \" s.j. \": \" jesuit \",\n",
    "    \" s j \": \" jesuit \",\n",
    "    \" s. j. \": \" jesuit \",\n",
    "    \"n.j.\": \"newjersey\",\n",
    "    \"n. j.\": \"newjersey\",\n",
    "    \"n.y.\": \"newyork\",\n",
    "    \"n. y.\": \"newyork\",\n",
    "    \" ny \": \" newyork \",\n",
    "    \"c.p.\": \"passionist\",\n",
    "    \"c. p.\": \"passionist\",\n",
    "    \"s.v.d.\": \"svd\",\n",
    "    \"s. v. d.\": \"svd\",\n",
    "    \" svd \": \" svd \",\n",
    "    \"k of c \": \"kofc \",\n",
    "    \"greek\": \"greece\",\n",
    "    \"greeks\": \"greece\", \n",
    "    \"washington d c\": \"washingtondc\", \n",
    "    \"washington d. c.\": \"washingtondc\", \n",
    "    \"washington d.c.\": \"washingtondc\", \n",
    "    \" rev \": \" priest \",\n",
    "    \" fr \": \" priest \",\n",
    "    \" rev. \": \" priest \",\n",
    "    \" fr. \": \" priest \",\n",
    "    \" hy \": \" by \", \n",
    "    \"yanllng\": \"yuanling\",\n",
    "    \"passlonlst\": \"passionist\",\n",
    "    \"passlonist\": \"passionist\",\n",
    "    \"passionlst\": \"passionist\",\n",
    "    \"jugoslavia\" : \"yugoslavia\",\n",
    "    \"indiaapolis\" : \"indianapolis\",\n",
    "    \"mlssioner\" : \"missioner\",\n",
    "    \"chlna\" : \"china\",\n",
    "    \"chima\" : \"china\",\n",
    "    \" xl \": \" xi \",\n",
    "    \"tslngtao\": \"tsingtao\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary below is a list of words to be removed from each article (in addition to a standard list of space words). This list was developed by myself using the same process described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = {'million', 'thousand', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "    'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen',\n",
    "    'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', \n",
    "    'year', 'years', 'month', 'day', 'days', 'yearly', 'monthly', 'daily', 'week', 'weekly', 'days', 'years', 'months', 'weeks', \n",
    "    'today', 'yesterday', 'tomorrow', 'night', 'nights',\n",
    "    'hour', 'hours', 'minute', 'minutes',\n",
    "    'first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'eleventh', \n",
    "    \"fifteenth\",\n",
    "    'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', \n",
    "    'jan', 'feb', 'mar', 'apr', 'may', 'aug', 'sept', 'oct', 'nov', 'dec',\n",
    "    'sursum', 'corda', 'halfh', 'thirty', 'ago', 'annual', \n",
    "    'sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', \n",
    "    'summer', 'winter', 'fall', 'spring', \n",
    "    'quarter', 'half', 'halves', 'quarters',\n",
    "    'pm', 'am', 'cst', 'est',\n",
    "    \"tha\", \"th\", \"la\", \"ha\", \"bt\", \"hi\", \"ho\", \"hie\", \"aa\", \"ths\", \"bo\", \"lo\", \"le\", \"aad\", \"te\", \"bt\", \"ha\", \"ara\", \"ba\", \"tba\",\n",
    "    'tte', 'tbs', 'tbo', 'mm', 'ad', 'oa', 'ii', 'ia', 'af', 'oo', 'ar', 'ta', 'al', 'si', 'de', \"tht\", \"tto\", \"bs\", \"tb\", \"ft\", \"iba\",\n",
    "    \"hla\", \"wwa\", \"haa\", \"ao\", \"ib\", \"da\", \"ef\", \"tor\", \"aro\", \"baa\", \"oro\", \"froa\", \"toa\", \"ml\", \"lha\", \"mi\", \"ti\", \"ay\", \"ml\", \"wat\", \n",
    "    \"fea\", \"fro\", \"waa\", \"wa\", \"thl\", \"thalr\", \"lon\", \"ae\", \"bov\", \"ot\", \"wae\", \"\", \"nr\", \"ll\", \"na\", \"lev\", \"ua\", \"ro\", \"biabar\", \"oei\",\n",
    "    \"thc\", \"oara\", \"hy\", \"iho\", \"oho\", \"bor\", \"tollo\", \"boro\", \"tta\", \"tod\", \"mu\", \"hor\", \"apo\", \"lata\", \"hwa\",\n",
    "    \"tbe\", \"til\", \"ssi\", \"tnat\", \"flret\", \"sha\", \"hlo\", \"rov\", \"jt\", \"sal\", \"bf\", \"lia\", \"ono\", \"tj\", \"pari\", \"ei\", \"flrat\", \"dm\", \"rc\",\n",
    "    \"wr\", \"har\", \"pl\", \"aald\", \"flr\", \"ke\", \"cu\", \"ub\", \"pln\", \"ser\", \"thia\", \"val\", \"burl\", \"ji\", \"nas\", \"les\", \"oc\", \"der\", \"ul\", \"ec\", \"fh\",\n",
    "    \"vl\", \"sn\", \"mw\", \"tut\", \"ru\", \"ber\", \"fhe\", \"pt\", \"ov\", \"ssr\", \"ret\", \"ds\", \"iw\", \"mac\", \"kt\", \"ag\", \"els\", \"je\", \"hoy\", \"loa\", \"hls\",\n",
    "    \"ami\", \"ow\", \"ko\", \"jm\", \"wno\", \"mn\", \"tw\", \"nf\", \"nh\", \"inf\", \"ior\", \"ew\", \"fer\", \"oot\", \"hj\", \"bn\", \"wn\", \"patna\", \"jh\", \"ij\", \"lh\",\n",
    "    \"nal\", \"lira\", \"ste\", \"prl\", \"und\", \"pal\", \"js\", \"ge\", \"pj\", \"mae\", \"cd\", \"oen\", \"tim\", \"ity\", \"tp\", \"ond\", \"eald\", \"ilf\", \"ito\", \"uc\",\n",
    "    \"pp\", \"nm\", \"taka\", \"aay\", \"ort\", \"iy\", \"lng\", \"iff\", \"ga\", \"wt\", \"sv\", \"croe\", \"laat\", \"iha\", \"thr\", \"tro\", \"sy\", \"sac\", \"bv\", \"rea\",\n",
    "    \"ame\", \"jp\", \"uu\", \"oon\", \"ud\", \"yoar\", \"td\", \"fj\", \"nev\", \"tse\", \"hn\", \"fs\", \"esse\", \"xt\", \"mj\", \"fre\", \"fha\", \"wor\", \"jn\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines the method *preprocess_entity*. This method cleans the text of one given article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(nlp.Defaults.stop_words)\n",
    "\n",
    "# preproccesing a single article\n",
    "def preprocess_entity(article):\n",
    "    # print(f'---- Original Article ----\\n{article}\\n------------------')\n",
    "    article = re.sub(r'\\s{2,}', ' ', article)\n",
    "    article = re.sub(r'([A-Z]\\. ?)+([A-Z])\\. ?', lambda match: match.group().replace(\".\", \"\").replace(\" \", \"\") + \" \", article)\n",
    "    article = article.lower().strip()\n",
    "    \n",
    "    article = contractions.fix(article)\n",
    "\n",
    "    # Replace terms in the string\n",
    "    for key, value in replace_terms.items():\n",
    "        article = article.replace(key, value)\n",
    "\n",
    "    article = re.sub(r'[^A-Za-z\\_]', ' ', article) \n",
    "    article = re.sub(r'\\s{2,}', ' ', article)\n",
    "    \n",
    "    # SpaCy tokenization\n",
    "    doc = nlp(article)\n",
    "\n",
    "    # Lemmatization and remove stopwords\n",
    "    processed_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    article = ' '.join(processed_tokens)\n",
    "\n",
    "    # Remove stop words and words of length 1\n",
    "    article = ' '.join(word for word in article.split() \n",
    "                       if word not in stop_words and \n",
    "                       len(word) > 1) \n",
    "    original_word_count = len(article.split())\n",
    "    \n",
    "    # Remove redundant words\n",
    "    \n",
    "    # hcvc, ncnc, iccw, nowc, usgr, nctc \n",
    "    article = ' '.join(word for word in article.split() \n",
    "                     if word not in words_to_remove)\n",
    "    remaining_word_count = len(article.split())\n",
    "    \n",
    "    # Get rid of the article if too many words were removed\n",
    "    if (original_word_count == 0) or (100 * remaining_word_count / original_word_count) < 50:\n",
    "        return None\n",
    "    \n",
    "    # Remove punctuation in abbreviations\n",
    "    article = re.sub(r'\\b([A-Z]\\. ?)+\\b', lambda match: match.group().replace('.','').replace(' ',''), article)\n",
    "    # print(f'---- Final Article ----\\n{article}\\n------------------')\n",
    "    return article\n",
    "\n",
    "def clean_article(article):\n",
    "    clean_article = preprocess_entity(article)\n",
    "    return clean_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing method is run on each article in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ThreadPoolExecutor for# Use ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    preprocessed_articles = list(executor.map(preprocess_entity, df['ocr_text_cleaned']))\n",
    "df['preprocessed_article'] = preprocessed_articles\n",
    "\n",
    "print(f'original number of articles: {len(df.index)}')\n",
    "df = df.dropna(subset=['preprocessed_article'], axis=0)\n",
    "print(f'final number of articles after word replacement: {len(df.index)}')\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    preprocessed_articles = list(executor.map(preprocess_entity, df['ocr_text_cleaned']))\n",
    "df['preprocessed_article'] = preprocessed_articles\n",
    "\n",
    "print(f'original number of articles: {len(df.index)}')\n",
    "df = df.dropna(subset=['preprocessed_article'], axis=0)\n",
    "print(f'final number of articles after word replacement: {len(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A frequency dictionary is built for occurances of words throughout the entire corpus. Words with low frequency are deleted. Articles that had too many words deleted are removed from the corpus. Then, the final data frame is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute frequencies\n",
    "FREQ_MIN = 10\n",
    "frequency = defaultdict(int)\n",
    "for text in df['preprocessed_article']:\n",
    "    for token in text.split(' '):\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Filter tokens with frequency less than FREQ_MIN\n",
    "frequency_filtered = {token: freq for token, freq in frequency.items() if freq <= FREQ_MIN}\n",
    "\n",
    "# Convert the filtered frequency dictionary to a DataFrame\n",
    "df_freq = pd.DataFrame(list(frequency_filtered.items()), columns=['token', 'frequency'])\n",
    "df_freq.to_csv('words_deleted.csv')\n",
    "\n",
    "def delete_low_freq_words(article):\n",
    "    global frequency_filtered\n",
    "    original_word_count = len(article.split())\n",
    "    article = ' '.join(word for word in article.split() if word not in frequency_filtered)\n",
    "    remaining_word_count = len(article.split())\n",
    "    \n",
    "    # Get rid of the article if too many words were removed\n",
    "    if (original_word_count == 0) or (100 * remaining_word_count / original_word_count) < 50:\n",
    "        return None\n",
    "    \n",
    "    return article\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    preprocessed_articles = list()\n",
    "    cleaned_articles = list(executor.map(delete_low_freq_words, df['preprocessed_article']))\n",
    "\n",
    "df['preprocessed_article'] = cleaned_articles\n",
    "df = df.dropna(subset=['preprocessed_article'], axis=0)\n",
    "print(f'final number of articles after word frequency test: {len(df.index)}')\n",
    "\n",
    "df.to_csv('cns_preprocessed_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cna_cst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
